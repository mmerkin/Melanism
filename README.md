# Melanism

The code and files used to compare melanic haplotypes across three species of geometrid moth.

Any words in all caps correspond to variables that need to be replaced with your data, unless otherwise specified.

Conda environment yaml files can be loaded with this command:

```bash
conda env create -n $NAME -f $FILE.yml
```

# Long read assembly

Required files: HiFi reads in fastq.gz format, HiC data in fastq.gz format, parental short reads for trio individuals in fastq.gz format

Required installations: yak, hifiasm, gfatools, yahs, compleasm, gaas, seqkit
Conda environment: assembly

### HIFI read assembly

1) Count the kmers from both parental read sets. Note that yak needs to be fed the same reads twice for paired end reads. 
```bash
yak count -b36 -t16 -o $OUTPUT.yak <(cat $PARENT_R1.fastq.gz $PARENT_R2.fastq.gz) <(cat $PARENT_R1.fastq.gz $PARENT_R2.fastq.gz)
```
2) Perform triobinning assembly with hifiasm
```bash
hifiasm -o $OUTPUT.asm -t32 -1 $PATERNAL.yak -2 $MATERNAL.yak $HIFI.fastq.gz
```
3) Perform HIFI-only assembly for individuals lacking paternal reads
```bash
hifiasm -o $OUTPUT.asm -t 32 $HIFI.fastq.gz
```
4) Convert the output gfa to a fasta file
```bash
gfatools gfa2fa $HAPLOTYPE.p_ctg.gfa > $HAPLOTYPE.fa
```
5) Calculate error scores for trio samples
```bash
yak trioeval -t16 $PATERNAL.yak $MATERNAL.yak $HAPLOTYPE.fa
```

### Reference genome construction

1) Filter and map HIC data using the [arima mapping pipeline](https://github.com/ArimaGenomics/mapping_pipeline)
2) Scaffold the contigs for Phigalia
```bash
yahs $HAPLOTYPE.fa $MAPPED_HIC.bam
```
3) Perform Odontopera scaffolding using haphic, which needs to be installed locally. The two different approaches were used as Phigalia scaffolding produced a single scaffold with haphic, whilst Odontopera had too many scaffolds with yahs.
```bash
/PATH/TO/haphic pipeline $HAPLOTYPE.fa $MAPPED_HIC.bam $CHROMOSOME_NUMBER --correct_nrounds 10 --threads 32
```
4) HIC-contact maps can be constructed using juicer from the yahs output, which is explained on the [yahs github](https://github.com/c-zhou/yahs?tab=readme-ov-file#generate-hic-contact-maps)
5) Obtain a list of scaffolds to keep from either the contact map or a GENESPACE riparian plot. For Ob, this is everything after group31; for Pp, this is everything after 117 and also 67 (which appears to be bacterial-> 
4Mb genome of *Pantoea anthophila*).
6) Remove unwanted scaffolds after creating a text file containing the names of the scaffolds to be removed (without the ">")
```bash
grep ">" $SPECIES_SCAFFOLDS.fa | sed 's/>//'
seqkit grep -v -n -f $UNWANTED_SCAFFOLDS.txt $SPECIES_SCAFFOLDS.fa > $SPECIES_GENOME.fa
```

### Sequencing statistics

1) Calculate compleasm score of genome completeness (similar to BUSCO)
```bash
compleasm run -a $HAPLOTYPE.fa -o $HAPLOTYPE_output -t 16 -l lepidoptera
```
2) Calculate more sequencing statistics with gaas
```bash
gaas_fasta_statistics.pl -f $HAPLOTYPE.fa
```

Final sequencing statistics can be found [here](assembly_info)

### Sequencing stats plots

1) Generate busco scores
```bash
busco -i $REF -o $OUTPUT -l lepidoptera_odb10 -m genome
```

2) Build blobtoolkit database. The tax id for Ob is 875882 and Pp is 326965
```bash
blobtools create --fasta $REF --taxid $TAXID --taxdump taxdump/ --busco $BUSCO/OUTPUT/PATH/full_table.tsv $OUTPUT
```
3) View the snail plot by clicking on the local host link generated by blobtools
```bash
blobtools view --remote $BLOBTOOLS_OUTPUT
```

# Read quality control

Required files: bam or fastq files

Required installations: fastqc and multiqc

Conda environment: filtering

1) Use fastqc to generate reports for each file
```bash
fastqc *
```
2) Use multiqc to merge these into a single report
```bash
multiqc *_fastqc.zip
```
3) View the average depth of each sample with the script [run_generate_depths.sh](scripts/run_generate_depths.sh)

# Pangenome-based variant calling

Required files: reference genome, long-read contigs, short reads

Required installations: cactus, pangenie, vcflib, bcftools, parallel

Conda environment: pangenie
Note: cactus had to be installed separately as explained [here](https://github.com/ComparativeGenomicsToolkit/cactus/blob/v2.9.2/BIN-INSTALL.md)

1) Create a file containing the names of the reference and contig names and their file paths, as shown [here](example_files/Ob_contigs.txt)
2) Use minigraph-cactus to create the pangenome
```bash
cactus-pangenome job_store $SPECIES_contigs.txt --outDir output_files --outName $SPECIES_pangenome --reference reference --gbz clip filter full --viz --gfa clip full --vcf --vcfReference reference --logFile ./SPECIES_pangenome.log --consCores 8
```
3) Remove overlapping variants, variants with more than 200 alleles and the info field that results in error messages later on
```bash
vcfwave -I 15000 -t 32 $SPECIES_pangenome.vcf > $SPECIES_pangenome.wave.vcf
bcftools norm -m- $SPECIES_pangenome.wave.vcf -o $SPECIES_pangenome_norm.vcf --threads 32
bcftools sort -o $SPECIES_pangenome_norm_sorted.vcf $SPECIES_pangenome_norm.vcf
vcfcreatemulti $SPECIES_pangenome_norm_sorted.vcf > $SPECIES_pangenome_multi.vcf
bcftools view --max-alleles 200 $SPECIES_pangenome_multi.vcf > $SPECIES_pangenome_reduced_multi.vcf
bcftools annotate -x INFO $SPECIES_pangenome_reduced_multi.vcf > $SPECIES_pangenome_noinfo_reduced_multi.vcf
```
4) Index the reference genome for pangenie
```bash
PATH/TO/PanGenie-index -v $SPECIES_pangenome_noinfo_reduced_multi.vcf -r $GENOME.fa -t 32 -o $SPECIES_pangenie
```
5) Call genotypes for short-read samples using the script [run_pangenie_genotyping.sh](scripts/run_pangenie_genotyping.sh)
6) Compress, index and merge all vcfs in the directory to produce a final vcf containing variants from each sample
```bash
parallel bgzip {} ::: *.vcf
for f in ./*.vcf.gz; do tabix -p vcf -f $f;done
bcftools merge *.vcf.gz --threads 32 -o $SPECIES_merged.vcf.gz
```
Note: parallel needs to be cited as Tange, O. (2024). GNU Parallel 20240722 ('Assange') [stable]. Zenodo. https://doi.org/10.5281/zenodo.12789352

# Association analysis

Required files: vcf

Required installations: plink

conda environment: plink

### SWAS (scaffold-wide association study)

1) Create a plink [phenotype file](example_files/Ob_phenotypes.txt)
2) Run the association analysis, filtering out individuals with more than 10% missing data and variants that are not called in more than 5% of individuals
```bash
plink --vcf $VCF --double-id --allow-extra-chr \
--set-missing-var-ids @:# --vcf-half-call m --allow-no-sex \
--geno 0.05 \
--pheno $PHENOTYPES.txt \
--assoc --out $OUTPUT_NAME
```
3) Download the assoc file and place it in the R working directory
4) Plot the data in R with ggplot
```R
library(tidyverse)

read_table("FILE.assoc") %>%
ggplot(.,aes(x=BP/1000000,y=-log10(P))) + geom_point() +theme_classic() +
  xlab("position (Mb)")
```
### Genotype plot

1) Extract the top n variants associated with the phenotype and create a new vcf. Pangenie can give multiple variants the same id, so both id and position information is used. Note that freebayes does not assign ids, so the position should be used only. The resulting vcf should also be sorted, compressed and indexed.
```bash
sort --key=8 -nr $FILE.assoc | head -n 25 > $FILE_first_25.txt
awk '{ print $2 }' $FILE_first_25.txt > $FILE_25_linked_snp_ids.txt
awk '{print $1"\t"$3}' $FILE_first_25.txt > $FILE_25_linked_snp_pos.txt

vcftools --snps $FILE_25_linked_snp_ids.txt --positions $FILE_25_linked_snp_pos.txt --vcf VCF --recode --out $FILE_25linked_snps
bcftools sort $FILE_25linked_snps.recode.vcf > $FILE_25_sorted.vcf
bgzip $FILE_25_sorted.vcf
tabix $FILE_25_sorted.vcf.gz
```
2) Download the gVCF and index files and move them to the R working directory
3) Make a [popmap csv file](example_files/Ob_popmap.csv)
4) Construct the genotype plot in R with [Create_genotype_plot.R](scripts/Create_genotype_plot.R)

# De novo variant calling

### Map and filter reads

Required files: reference genome, short reads in fastq.gz format

Required installations: bwa-mem2, samtools

conda environment: mem2

1) Index the refence genome
```bash
bwa-mem2 index $REF.fa
```
2) Run the script [run_map_reads.sh](scripts/run_map_reads.sh) to produce filtered bam and bam.bai files, replacing the variables at the top. This removes unmapped and secondary reads, then removes duplicate reads. A mate-score tag is added with "fixmate" and the read with the lowest score is removed. 

### Produce vcf file

Required files: reference genome, read alignment bam files

Required installations: bamadrrg, freebayes, parallel

conda environment: freebayes


1) Merge all of the bam files together with bamaddrg.sh
2) Run freebayes in parallel to call variants with the best 2 alleles.
```bash
freebayes-parallel <(fasta_generate_regions.py $REF.fa.fai 100000) $THREADS -f $REF --use-best-n-alleles 2 "$MERGED_bamaddrg.bam" > $SPECIES_2alleles_freebayes.vcf
```
3) Rename samples by removing file path info and the file suffix 
```bash
vcf-query -l $VCF | sed -e 's:^.*/::' -e 's:.processed.bam::' > $NEW_NAMES.txt
bcftools reheader -s $NEW_NAMES.txt -o $SPECIES_freebayes.vcf.gz  $SPECIES_2alleles_freebayes.vcf.gz
```

### Filtered vcf analysis

1) Run the script [run_filter_vcfs.sh](scripts/run_filter_reads.sh] to thoroughly filter the vcfs for the pca. A threshold value should be specified with which to filter out individuals based on their missingness. This was determined by produced the pca plot and identifying any outliers. If the outliers were the samples with the highest missingness, the threshold was lowered to remove these samples until this was no longer the case. For reference, the missingness thresholds chosen for Ob was 0.3, Pp was 0.025 and Ob was 0.45.
2) Download the plink files ending in ".eigenvec" and ".eigenval" and run the R script [Create_PCA_plot.R]
3) Reperform the association analysis in the same manner as for the pangenie vcfs, using the filtered vcfs created by freebayes instead. 


# Scrapped ideas

The following analyses were attempted, but not used in the manuscript. This was primarily due to concerns that the data were not comparable across species eg Biston betularia samples were taken from too narrow of a range for IBD and sequence capture targeted too small of a region to identify sweep signatures. 

###Isolation-by-distance (IBD)

Required files: vcf, coordinates of each sample site

Required installations: vcftools

1) Create files listing the members of each population in the format SPECIESID_POP_samples.txt as in the [example](example_files/Ob_MTW_samples.txt). A list of all sample names can be created with vcftools using
```bash
vcf-query -l VARIANTS.vcf > all_samples.txt
```
2) Make a list of all populations to be compared separated by either a tab or space
3) Run the script [run_generate_pairwise_fst.sh](scripts/run_generate_pairwise_fst.sh), ensuring that the variables are replaced
4) Download the following output files and place these in your R working directory

SPECIESID_pairwise_comparisons.txt contains a list of all the pairwise comparisons

SPECIESID_mean_fsts.txt contains the mean whole genome fst values for each pairwise comparison

SPECIESID_weighted_fsts.txt contains the weighted whole genome fst values for each pairwise comparison

5) Create a csv file containing the coordinates of each sample site in UK grid coordinates as per the [example](example_files/Ob_coordinates.csv)
6) Run the R script [Create_IBD_plot.R](scripts/Create_IBD_plot.R)

# Sweep signatures

lassip code WIP

```bash
bcftools reheader -s Ppfb_newnames.txt -o Ppfb_renamed.vcf  ~/pub64/mattm/vcf_analysis/SNPfiltering/Pp_attempt3/Ppfb_allfilters2.recode.vcf


cat Ppfb_newnames.txt | awk '{ print $1 "\tPp" }' > Pp_popmap.txt


#! /bin/bash

VCF=Ppfb_renamed.vcf
popmap=Pp_popmap.txt

for window in 2500 2250 2000 1750 1500 1250 1000; do
./lassip-v1.2.0 --vcf $VCF --threads 64 --hapstats --lassi --out "${filetag}_${window}win" --winsize $window --winstep 500 --pop $popmap
done

for window in 900 800 700 600 500 400 300 200; do
./lassip-v1.2.0 --vcf $VCF --threads 64 --hapstats --lassi --out "${filetag}_${window}win" --winsize $window --winstep 100 --pop $popmap
done

```

# Extra

Here is a list of each melanic individual and which haplotype they possess

## Odontopera

### nigra primary haplotype

Sample_11_89-Ob_RWD_2004_33_n
Sample_11_78-Ob_PRS_2013_11_n
Sample_11_79-Ob_PRS_2013_12_n
Sample_11_76-Ob_PRS_2013_09_n
Sample_11_13-SWN_2011_mel_male
Sample_11_77-Ob_PRS_2013_10_n
Sample_10_93-Ob_morecombe_2013_01_n
Sample_11_48-BAS_2013_03_n
Sample_11_80-Ob_PRS_2013_13_n
Sample_11_74-Ob_PRS_2013_07_n
Sample_11_75-Ob_PRS_2013_08_n
Sample_11_43-Heath_Charnock_2013_01_n
nigra_trio
Sample_11_5-MAL_2011_05_n

### nigra secondary haplotype

Sample_10_94-OB_MT_2013_01_n
Sample_11_32-MTW_2013_12_n
nigra_alt (Ob_MTW_2013_11n)
Sample_11_14-Swiths_Rd_Leics_2011_n
Sample_11_61-Ob_MLT_2004_02_n

### nigra other haplotype

Sample_11_81-Ob_KLD_2013_01_n
Sample_11_67-Ob_CE1_2004_02_n
Sample_11_17-Ob_fam15_F1N_2
Sample_11_15-Ormston_2011_n
Sample_11_16-Ob_fam15_F1N_1

## Phigalia

### Monacharia primary haplotype
	
Sample_10_31-Pp_IHW_2012_14_m
Sample_10_28-Pp_IHW_2012_11_m
Sample_10_63-Pp_IHW_2014_03_m
Sample_10_90-Pp_IHW_2012_05_m
Sample_10_91-Pp_IHW_2012_07_m
monacharia_trio
Sample_10_87-Pp_IHW_2012_02_m
Sample_10_27-Pp_IHW_2012_10_m

### Monacharia secondary haplotype
	
monacharia_alt (13_Pp_IHW_2012_09_M)
Sample_10_88-Pp_IHW_2012_03_m
Sample_10_8-PP_ARG_2013_08_m

### Monacharia other haplotypes

Sample_10_29-Pp_IHW_2012_12_m
Sample_10_86-Pp_IHW_2012_01_m
